<!doctype html>
<html>
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <title>Extremely fast decompression library? (word-aligned) - gbadev.org forum archive</title>
        <link rel="stylesheet" href="/static/pure-min.css" />
        <link rel="stylesheet" href="/static/main.css" />
    </head>
    <body>
        <h1>gbadev.org forum archive</h1>

        This is a mirror of the content originally found on forum.gbadev.org
        (now offline), salvaged from Wayback machine copies. <br />

        <h2>Coding > Extremely fast decompression library? (word-aligned)</h2>
<div id="posts">
<div class="post">
    <h4>#80741 - Dwedit - Tue Apr 25, 2006 4:20 am</h4>
    <div class="postbody"><span class="postbody">Does anyone know of an extremely fast decompression library that operates at the word level (32 bit) and no lower?
<br/>
<br/>
Something that would be decompressed like this or similar:
<br/>
</span><table align="center" border="0" cellpadding="3" cellspacing="1" width="90%"><tr> <td><span class="genmed"><b>Code:</b></span></td> </tr> <tr> <td class="code">
<br/>
void decompress(int *s, int *d)
<br/>
{
<br/>
   int dist,size;
<br/>
   while (1)
<br/>
   {
<br/>
      dist=*s++;
<br/>
      if (dist&gt;0)
<br/>
      {
<br/>
         do //raw words
<br/>
         {
<br/>
            *d++=*s++;
<br/>
         } while (--dist);
<br/>
      }
<br/>
      else if (dist&lt;0)
<br/>
      {
<br/>
         size=*s++;
<br/>
         do //previously seen words
<br/>
         {
<br/>
            *d=d[dist];
<br/>
            d++;
<br/>
         } while (--size);
<br/>
      }
<br/>
      else
<br/>
      {
<br/>
         return;
<br/>
      }
<br/>
   }
<br/>
}
<br/>
</td> </tr></table><span class="postbody">
<br/>
Never mind that this would obviously be poor compression...
<br/>
<br/>
If one doesn't exist, does anyone know of an easy-to-understand LZ compression library that could be modified to be like that?<br/>_________________<br/>"We are merely sprites that dance at the beck and call of our button pressing overlord."</span><span class="gensmall"></span></div>    
</div>
<div class="post">
    <h4>#80742 - poslundc - Tue Apr 25, 2006 4:34 am</h4>
    <div class="postbody"><span class="postbody">I'm not really aware of any good compression algorithms that aren't "byte-or-smaller".
<br/>
<br/>
Part of this I'm sure is just that the age of most of the algorithms out there predate 32-bit processors, but another large part of it also has to do that when you are trying to compress data - and especially smaller samples of data - you trade off in effectiveness by enforcing rules such as word-alignment.
<br/>
<br/>
In most compression schemes, the odds of having the same random byte repeated (1 in 256) is far more significant than the odds of having four random bytes repeated (1 in over four billion, and that's alignment notwithstanding).
<br/>
<br/>
Still, it's a good question, though, and if I ever were to go back for my Ph. D. it might be to try and devise an optimal 32-bit compression scheme for meeting or beating the bit/byte-level ones.
<br/>
<br/>
Dan.</span><span class="gensmall"></span></div>    
</div>
<div class="post">
    <h4>#80810 - Miked0801 - Tue Apr 25, 2006 7:15 pm</h4>
    <div class="postbody"><span class="postbody">RLE style algorithms can be changed to word alignment fairly easily.  Same for Huffman style compression, but both almost always lose quite a bit of efficiency in the process.  What are you doing that requires word aligned decompression?</span><span class="gensmall"></span></div>    
</div>
</div>

    </body>
</html>
